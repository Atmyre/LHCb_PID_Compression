{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
    "config.gpu_options.visible_device_list = '0'\n",
    "\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, Lambda, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.objectives import binary_crossentropy, mse\n",
    "from keras.losses import mean_squared_error\n",
    "from keras import backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "tf.set_random_seed(1)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_two_head_gen(input_shape, noise_shape, n_layers, thickness, encoding_dim, optimizer):\n",
    "    input_main = Input(shape=(input_shape, ), name='input_main')\n",
    "    input_noise = Input(shape=(noise_shape, ), name='input_noise')\n",
    "    \n",
    "    x = keras.layers.concatenate([input_noise, input_main])\n",
    "    \n",
    "    for i in range(n_layers - 1):\n",
    "        x = Dense(thickness*2, activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = keras.layers.concatenate([x, input_main])\n",
    "        \n",
    "    y_1 = Dense(thickness, activation='relu')(x)\n",
    "    y_1 = BatchNormalization()(y_1)\n",
    "    y_1 = keras.layers.concatenate([y_1, input_main])\n",
    "    generated_1 = Dense(encoding_dim, name='generated_1')(y_1)\n",
    "    \n",
    "    \n",
    "    y_2 = Dense(thickness, activation='relu')(x)\n",
    "    y_2 = BatchNormalization()(y_2)\n",
    "    y_2 = keras.layers.concatenate([y_2, input_main])\n",
    "    generated_2 = Dense(encoding_dim, name='generated_2')(y_2)\n",
    "    \n",
    "    \n",
    "    gen_1 = Model([input_noise, input_main], generated_1, name=\"gen_1\")\n",
    "    gen_2 = Model([input_noise, input_main], generated_2, name=\"gen_2\")\n",
    "    \n",
    "#     optimizer_adam_1 = optimizers.Adam(lr=0.001)\n",
    "    gen_1.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "#     optimizer_adam_2 = optimizers.Adam(lr=0.001)\n",
    "    gen_2.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return gen_1, gen_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminator(input_shape, alpha_shape, n_layers, thickness, optimizer):\n",
    "    input_alpha = Input(shape=(alpha_shape, ), name='input_main')\n",
    "    inputs = Input(shape=(input_shape, ), name='input')\n",
    "    \n",
    "    x = keras.layers.concatenate([inputs, input_alpha])\n",
    "    \n",
    "    for i in range(n_layers - 1):\n",
    "        x = Dense(thickness*2, activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = keras.layers.concatenate([x, input_alpha])\n",
    "        \n",
    "    out = Dense(1, name='out', activation='sigmoid')(x)\n",
    "    \n",
    "    disc = Model([inputs, input_alpha], out, name=\"disc\")\n",
    "    \n",
    "#     optimizer_adam = optimizers.Adam(lr=0.001)\n",
    "    disc.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gan_network(generator, discriminator, input_shape, noise_shape, optimizer):\n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    gan_input_main = Input(shape=(input_shape,))\n",
    "    gan_input_noise = Input(shape=(noise_shape,))\n",
    "    x = generator([gan_input_noise, gan_input_main])\n",
    "    gan_output = discriminator([x, gan_input_main] )\n",
    "    \n",
    "    gan = Model(inputs=[gan_input_noise, gan_input_main], outputs=gan_output)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input(uniform_bounds, uniform_dim, normal_dim, batch_size):\n",
    "    alpha = np.random.uniform(*uniform_bounds, size=uniform_dim)\n",
    "    return alpha, np.array([np.random.normal(alpha, 1, size=normal_dim) for i in range(batch_size)])\n",
    "\n",
    "def generate_data(bounds, batch_size, normal_dim=1000, num=10000):\n",
    "    alphas = []\n",
    "    datas = []\n",
    "\n",
    "    for i in range(num):\n",
    "        alpha, data = generate_input(bounds, 1, normal_dim, batch_size)\n",
    "        alphas.append(alpha)\n",
    "        datas.append(data)\n",
    "\n",
    "    return alphas, datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha_01, data_01 = generate_data([3, 4], 256)\n",
    "# np.save(\"alpha_01\", alpha_01)\n",
    "# np.save(\"data_01\", data_01)\n",
    "\n",
    "# alpha_02, data_02 = generate_data([0, 2], 256)\n",
    "# np.save(\"alpha_02\", alpha_02)\n",
    "# np.save(\"data_02\", data_02)\n",
    "\n",
    "# alpha_12, data_12 = generate_data([1, 2], 256, num=1000)\n",
    "# np.save(\"alpha_12\", alpha_12)\n",
    "# np.save(\"data_12\", data_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_01 = np.load(\"alpha_01.npy\")[:5000]\n",
    "data_01 = np.load(\"data_01.npy\")[:5000]\n",
    "\n",
    "alpha_02 = np.load(\"alpha_02.npy\")[:5000]\n",
    "data_02 = np.load(\"data_02.npy\")[:5000]\n",
    "\n",
    "alpha_12 = np.load(\"alpha_12.npy\")[:5000]\n",
    "data_12 = np.load(\"data_12.npy\")[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_01.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(alphas, datas, split=False, batch_size=64):\n",
    "    real_batch_size = len(datas) / len(alphas)\n",
    "    \n",
    "    batch_alpha = []\n",
    "    batch_data = []\n",
    "    \n",
    "    for i, alpha in enumerate(alphas):\n",
    "        for j in range(datas.shape[1]//batch_size):\n",
    "            batch_alpha.append([alpha]*batch_size)\n",
    "            batch_data.append(datas[i][j*batch_size:(j+1)*batch_size])\n",
    "            \n",
    "    x = np.random.permutation(len(batch_alpha))\n",
    "            \n",
    "    if not split:\n",
    "        return np.array(batch_alpha)[x], np.array(batch_data)[x]\n",
    "    else:\n",
    "        if 0 <= split <= 1:\n",
    "            size = len(np.array(batch_alpha))\n",
    "            return np.array(batch_alpha)[x][int(split*size):], np.array(batch_data)[x][int(split*size):], \\\n",
    "                   np.array(batch_alpha)[x][:int(split*size)], np.array(batch_data)[x][:int(split*size)]\n",
    "        else:\n",
    "            return np.array(batch_alpha)[x][split:], np.array(batch_data)[x][split:], \\\n",
    "                   np.array(batch_alpha)[x][:split], np.array(batch_data)[x][:split]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_alpha_01, batch_data_01, valid_alpha_01, valid_data_01 = prepare_data(alpha_01, data_01, split=1000)\n",
    "batch_alpha_02, batch_data_02, valid_alpha_02, valid_data_02 = prepare_data(alpha_02, data_02, split=1000)\n",
    "batch_alpha_12, batch_data_12 = prepare_data(alpha_12, data_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19000, 64, 1), (1000, 64, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_alpha_02.shape, valid_alpha_02.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19000, 64, 1000), (1000, 64, 1000))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data_01.shape, valid_data_01.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = 1\n",
    "INPUT_NOISE = 1000\n",
    "N_LAYERS = 3\n",
    "THICKNESS = 512\n",
    "ENCODING_DIM = 1000\n",
    "\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.stats\n",
    "def compare_data_to_dist(x, mus, sd=1):\n",
    "    ll = []\n",
    "    for i, item in enumerate(x):\n",
    "        ll_1 = 0\n",
    "        for j in item:\n",
    "            ll_1 += np.log(scipy.stats.norm(mus[i], sd).pdf(j))\n",
    "        ll.append(ll_1)\n",
    "    \n",
    "    print (\"The LL is: %.4f\" % (np.mean(ll)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.Adam(lr=0.0001)\n",
    "\n",
    "generator_1, generator_2 = get_two_head_gen(input_shape=INPUT_SHAPE, noise_shape=INPUT_NOISE, \n",
    "                             n_layers=N_LAYERS, thickness=THICKNESS, encoding_dim=ENCODING_DIM, optimizer=opt)\n",
    "discriminator = get_discriminator(input_shape=ENCODING_DIM, alpha_shape=INPUT_SHAPE, n_layers=N_LAYERS, thickness=THICKNESS, optimizer=opt)\n",
    "gan_1 = get_gan_network(generator_1, discriminator, input_shape=INPUT_SHAPE, noise_shape=INPUT_NOISE, optimizer=opt)\n",
    "gan_2 = get_gan_network(generator_2, discriminator, input_shape=INPUT_SHAPE, noise_shape=INPUT_NOISE, optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(batch_alpha, batch_data):\n",
    "    while True:\n",
    "        rp = np.random.permutation(len(batch_alpha))\n",
    "        for x, y in zip(batch_alpha[rp], batch_data[rp]):\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "generators = [generator_1, generator_2]\n",
    "gans = [gan_1, gan_2]\n",
    "uniform_bounds = [[0, 1], [0, 2]]\n",
    "\n",
    "alphas_valid = [np.concatenate(np.squeeze(valid_alpha_01)[:100]), np.concatenate(np.squeeze(valid_alpha_02)[:100])]\n",
    "batches_valid = [np.concatenate(valid_data_01[:100]), np.concatenate(valid_data_02[:100])]\n",
    "\n",
    "alpha_valid = np.concatenate(batch_alpha_12[:100])\n",
    "batch_valid = np.concatenate(batch_data_12[:100])\n",
    "\n",
    "wds = {\"01\": [], \"02\": [], \"12\": []}\n",
    "dls = {\"01\": [], \"02\": [], \"12\": []}\n",
    "\n",
    "iters_disc = 1\n",
    "iters_gen=5\n",
    "\n",
    "def valid(num):\n",
    "    noise = np.random.normal(0, 1, size=[len(alphas_valid[0]), INPUT_NOISE])\n",
    "#     alpha, real = generate_input(uniform_bounds[num], INPUT_SHAPE, ENCODING_DIM, batch_size)\n",
    "\n",
    "    generated = generators[num].predict([noise, alphas_valid[num]])\n",
    "    wd = np.mean([scipy.stats.wasserstein_distance(r, g) for r, g in zip(batches_valid[num], generated)])\n",
    "    print(\"WD\", wd)\n",
    "    wds[\"0\"+str(num+1)].append(wd)\n",
    "    \n",
    "    to_desc = np.concatenate([batches_valid[num], generated])\n",
    "\n",
    "    y_dis = np.zeros(2*len(alphas_valid[num]))\n",
    "    y_dis[:len(alpha_valid)] = 1\n",
    "\n",
    "    # Train discriminator\n",
    "    discriminator.trainable = False\n",
    "    dl = discriminator.evaluate([to_desc, np.concatenate([alphas_valid[num], alphas_valid[num]])], y_dis)\n",
    "    print(\"DL\", dl)\n",
    "    dls[\"0\"+str(num+1)].append(dl)\n",
    "    \n",
    "    if num == 0:\n",
    "        print(\"*****VAL [1, 2]*****\")\n",
    "        \n",
    "        generated = generators[num].predict([noise, alpha_valid])\n",
    "        wd = np.mean([scipy.stats.wasserstein_distance(r, g) for r, g in zip(batch_valid, generated)])\n",
    "        print(\"WD\", wd)\n",
    "        wds[\"12\"].append(wd)\n",
    "\n",
    "        to_desc = np.concatenate([batch_valid, generated])\n",
    "\n",
    "        y_dis = np.zeros(2*len(alpha_valid))\n",
    "        y_dis[:len(alpha_valid)] = 1\n",
    "\n",
    "        # Train discriminator\n",
    "        discriminator.trainable = False\n",
    "        dl = discriminator.evaluate([to_desc, np.concatenate([alpha_valid, alpha_valid])], y_dis)\n",
    "        print(\"DL\", dl)\n",
    "        dls[\"12\"].append(dl)\n",
    "        \n",
    "        print(\"*****VAL [1, 2]*****\")\n",
    "        \n",
    "def train_desc(num, batch_alpha, batch_data):\n",
    "    batch_size = batch_alpha.shape[0]\n",
    "    noise = np.random.normal(0, 1, size=[batch_size, INPUT_NOISE])\n",
    "\n",
    "    generated = generators[num].predict([noise, batch_alpha])\n",
    "    to_desc = np.concatenate([batch_data, generated])\n",
    "\n",
    "    y_dis = np.zeros(2*batch_size)\n",
    "    y_dis[:batch_size] = 1\n",
    "\n",
    "    discriminator.trainable = True\n",
    "    generators[0].trainable = False\n",
    "    generators[1].trainable = False\n",
    "    discriminator.train_on_batch([to_desc, np.concatenate([batch_alpha, batch_alpha])], y_dis)\n",
    "    \n",
    "def train_gen(num, batch_alpha, batch_data):\n",
    "    batch_size = batch_alpha.shape[0]\n",
    "    noise = np.random.normal(0, 1, size=[batch_size, INPUT_NOISE])\n",
    "    y_gen = np.ones(batch_size)\n",
    "    generators[num].trainable = True\n",
    "    discriminator.trainable = False\n",
    "    gans[num].train_on_batch([noise, batch_alpha], y_gen)\n",
    "\n",
    "def train(epochs=100, batch_size=64):\n",
    "    # Get the training and testing data\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_gen_01 = train_generator(np.squeeze(batch_alpha_01), batch_data_01)\n",
    "        train_gen_02 = train_generator(np.squeeze(batch_alpha_02), batch_data_02)\n",
    "        \n",
    "        for i in range(iters_disc):\n",
    "            batch_alpha, batch_data = next(train_gen_01)\n",
    "            train_desc(0, batch_alpha, batch_data)\n",
    "            \n",
    "        for i in range(iters_gen):\n",
    "            batch_alpha, batch_data = next(train_gen_01)\n",
    "            train_gen(0, batch_alpha, batch_data)\n",
    "            \n",
    "        for i in range(iters_disc):\n",
    "            batch_alpha, batch_data = next(train_gen_02)\n",
    "            train_desc(1, batch_alpha, batch_data)\n",
    "            \n",
    "        for i in range(iters_gen):\n",
    "            batch_alpha, batch_data = next(train_gen_02)\n",
    "            train_gen(1, batch_alpha, batch_data)\n",
    "            \n",
    "        if epoch % 10 == 0:\n",
    "            print()\n",
    "            print ('-'*15, 'Epoch %d' % epoch, '-'*15)\n",
    "            valid(0)\n",
    "            valid(1)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Epoch 10 ---------------\n",
      "WD 0.4862878638925723\n",
      "12800/12800 [==============================] - 1s 81us/step\n",
      "DL 0.6197108859653919\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.5564482759779366\n",
      "12800/12800 [==============================] - 1s 70us/step\n",
      "DL 0.00011485207516983564\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9537903814733503\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 0.3082428901938594\n",
      "\n",
      "\n",
      "--------------- Epoch 20 ---------------\n",
      "WD 0.48366336237444846\n",
      "12800/12800 [==============================] - 1s 62us/step\n",
      "DL 1.0295019218231807\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.55376082219224\n",
      "12800/12800 [==============================] - 1s 62us/step\n",
      "DL 3.0459489059353473e-06\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9518647788150522\n",
      "12800/12800 [==============================] - 1s 69us/step\n",
      "DL 0.4984546814593361\n",
      "\n",
      "\n",
      "--------------- Epoch 30 ---------------\n",
      "WD 0.4828773968764494\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 1.271882554247202\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.5528677851506705\n",
      "12800/12800 [==============================] - 1s 64us/step\n",
      "DL 9.431070442644795e-07\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.949842943436541\n",
      "12800/12800 [==============================] - 1s 67us/step\n",
      "DL 0.6169080721513847\n",
      "\n",
      "\n",
      "--------------- Epoch 40 ---------------\n",
      "WD 0.4814660609894088\n",
      "12800/12800 [==============================] - 1s 64us/step\n",
      "DL 1.3529039262236906\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.5507839540538475\n",
      "12800/12800 [==============================] - 1s 66us/step\n",
      "DL 7.478835345864354e-07\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9478636800890173\n",
      "12800/12800 [==============================] - 1s 67us/step\n",
      "DL 0.6585824524302314\n",
      "\n",
      "\n",
      "--------------- Epoch 50 ---------------\n",
      "WD 0.4793212265819078\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 1.2793175711852967\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.5488974609731434\n",
      "12800/12800 [==============================] - 1s 66us/step\n",
      "DL 8.748254903423458e-07\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9463046110634196\n",
      "12800/12800 [==============================] - 1s 67us/step\n",
      "DL 0.6185061586165398\n",
      "\n",
      "\n",
      "--------------- Epoch 60 ---------------\n",
      "WD 0.47818264433730023\n",
      "12800/12800 [==============================] - 1s 64us/step\n",
      "DL 1.4316789074173724\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.546266178272482\n",
      "12800/12800 [==============================] - 1s 68us/step\n",
      "DL 8.084164203125966e-07\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9446394216329257\n",
      "12800/12800 [==============================] - 1s 63us/step\n",
      "DL 0.7056008367974939\n",
      "\n",
      "\n",
      "--------------- Epoch 70 ---------------\n",
      "WD 0.47674463195426753\n",
      "12800/12800 [==============================] - 1s 68us/step\n",
      "DL 1.3993429033573248\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.5444708080770118\n",
      "12800/12800 [==============================] - 1s 64us/step\n",
      "DL 8.699044332161066e-07\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9423808896715642\n",
      "12800/12800 [==============================] - 1s 68us/step\n",
      "DL 0.6885863326735078\n",
      "\n",
      "\n",
      "--------------- Epoch 80 ---------------\n",
      "WD 0.47581613232900216\n",
      "12800/12800 [==============================] - 1s 66us/step\n",
      "DL 1.3355561337485076\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.5437156708603703\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 1.012181288899683e-06\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9408447960166643\n",
      "12800/12800 [==============================] - 1s 70us/step\n",
      "DL 0.6598744836303746\n",
      "\n",
      "\n",
      "--------------- Epoch 90 ---------------\n",
      "WD 0.47402372905005785\n",
      "12800/12800 [==============================] - 1s 64us/step\n",
      "DL 1.2180394321582186\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.5407766419603712\n",
      "12800/12800 [==============================] - 1s 67us/step\n",
      "DL 1.5220591589582e-06\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9396675014420164\n",
      "12800/12800 [==============================] - 1s 66us/step\n",
      "DL 0.5953513821334543\n",
      "\n",
      "\n",
      "--------------- Epoch 100 ---------------\n",
      "WD 0.4722682892391007\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 1.2393666438123976\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.5388560111443628\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 1.2855136778000541e-06\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9376365511959935\n",
      "12800/12800 [==============================] - 1s 66us/step\n",
      "DL 0.6044678471757838\n",
      "\n",
      "\n",
      "--------------- Epoch 110 ---------------\n",
      "WD 0.4709674194486109\n",
      "12800/12800 [==============================] - 1s 64us/step\n",
      "DL 1.2662125952375483\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.5368208063881394\n",
      "12800/12800 [==============================] - 1s 68us/step\n",
      "DL 1.1759536958777518e-06\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9362929261799507\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 0.618144073568415\n",
      "\n",
      "\n",
      "--------------- Epoch 120 ---------------\n",
      "WD 0.4701814113398673\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 1.344630416370872\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.5351101698124154\n",
      "12800/12800 [==============================] - 1s 70us/step\n",
      "DL 9.252985515217916e-07\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9346208192597609\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 0.6574825756431703\n",
      "\n",
      "\n",
      "--------------- Epoch 130 ---------------\n",
      "WD 0.4689825640343862\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 1.31498868712479\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.5329140666906764\n",
      "12800/12800 [==============================] - 1s 66us/step\n",
      "DL 1.4876564830501594e-06\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9324410797489355\n",
      "12800/12800 [==============================] - 1s 66us/step\n",
      "DL 0.6430074204233811\n",
      "\n",
      "\n",
      "--------------- Epoch 140 ---------------\n",
      "WD 0.4665559230648823\n",
      "12800/12800 [==============================] - 1s 66us/step\n",
      "DL 1.3658336557125261\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.5309907344355518\n",
      "12800/12800 [==============================] - 1s 64us/step\n",
      "DL 1.1935600912238442e-06\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9319626334156296\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 0.6716783283458791\n",
      "\n",
      "\n",
      "--------------- Epoch 150 ---------------\n",
      "WD 0.46604962914850334\n",
      "12800/12800 [==============================] - 1s 63us/step\n",
      "DL 1.589772046868812\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.5295192268367304\n",
      "12800/12800 [==============================] - 1s 66us/step\n",
      "DL 6.274792205829272e-07\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9287239837112131\n",
      "12800/12800 [==============================] - 1s 69us/step\n",
      "DL 0.8015199902672014\n",
      "\n",
      "\n",
      "--------------- Epoch 160 ---------------\n",
      "WD 0.4641830610275602\n",
      "12800/12800 [==============================] - 1s 63us/step\n",
      "DL 1.5404688607512709\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.5264392921430872\n",
      "12800/12800 [==============================] - 1s 64us/step\n",
      "DL 6.27323900630472e-07\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9267882124261715\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 0.7705105201444289\n",
      "\n",
      "\n",
      "--------------- Epoch 170 ---------------\n",
      "WD 0.4623167497922327\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 1.556973034025579\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.5242478071575363\n",
      "12800/12800 [==============================] - 1s 68us/step\n",
      "DL 5.777941231599471e-07\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9263143774598254\n",
      "12800/12800 [==============================] - 1s 64us/step\n",
      "DL 0.7744605423121931\n",
      "\n",
      "\n",
      "--------------- Epoch 180 ---------------\n",
      "WD 0.4615841809743034\n",
      "12800/12800 [==============================] - 1s 64us/step\n",
      "DL 1.5480088344090495\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.522428601999134\n",
      "12800/12800 [==============================] - 1s 62us/step\n",
      "DL 5.557806121458953e-07\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9234361948814296\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 0.7666262643700665\n",
      "\n",
      "\n",
      "--------------- Epoch 190 ---------------\n",
      "WD 0.460467489417898\n",
      "12800/12800 [==============================] - 1s 68us/step\n",
      "DL 1.5817034350721253\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.519937995576507\n",
      "12800/12800 [==============================] - 1s 63us/step\n",
      "DL 5.262033495867513e-07\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9224640643320344\n",
      "12800/12800 [==============================] - 1s 67us/step\n",
      "DL 0.7783057121461904\n",
      "\n",
      "\n",
      "--------------- Epoch 200 ---------------\n",
      "WD 0.4580999418127096\n",
      "12800/12800 [==============================] - 1s 66us/step\n",
      "DL 1.6902657309780447\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.5178900615131392\n",
      "12800/12800 [==============================] - 1s 61us/step\n",
      "DL 3.5434477130280584e-07\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9201312763182073\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 0.8375713604376736\n",
      "\n",
      "\n",
      "--------------- Epoch 210 ---------------\n",
      "WD 0.4574951290803089\n",
      "12800/12800 [==============================] - 1s 64us/step\n",
      "DL 1.7910392174962515\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.5160652550516298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 70us/step\n",
      "DL 3.9179431389868567e-07\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9187617257549691\n",
      "12800/12800 [==============================] - 1s 63us/step\n",
      "DL 0.9178812964783478\n",
      "\n",
      "\n",
      "--------------- Epoch 220 ---------------\n",
      "WD 0.45599293931976786\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 1.780466257532728\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.5137475889530585\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 1.0866248068452932e-06\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9164300717846643\n",
      "12800/12800 [==============================] - 1s 68us/step\n",
      "DL 0.9625330388878386\n",
      "\n",
      "\n",
      "--------------- Epoch 230 ---------------\n",
      "WD 0.45452003246092887\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 1.6446999146985692\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.5119988422139485\n",
      "12800/12800 [==============================] - 1s 63us/step\n",
      "DL 1.031484882219047e-06\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9148919133597683\n",
      "12800/12800 [==============================] - 1s 67us/step\n",
      "DL 0.8781507357994888\n",
      "\n",
      "\n",
      "--------------- Epoch 240 ---------------\n",
      "WD 0.4536049175083427\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 1.5764509993025195\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.5108879838567346\n",
      "12800/12800 [==============================] - 1s 63us/step\n",
      "DL 8.48232965608986e-07\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9146282118121326\n",
      "12800/12800 [==============================] - 1s 62us/step\n",
      "DL 0.8236952412407689\n",
      "\n",
      "\n",
      "--------------- Epoch 250 ---------------\n",
      "WD 0.45175642436357394\n",
      "12800/12800 [==============================] - 1s 64us/step\n",
      "DL 1.5930183216365728\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.5085585625154396\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 8.583226762937101e-07\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9116911401944955\n",
      "12800/12800 [==============================] - 1s 63us/step\n",
      "DL 0.8237029676768305\n",
      "\n",
      "\n",
      "--------------- Epoch 260 ---------------\n",
      "WD 0.4520607543501305\n",
      "12800/12800 [==============================] - 1s 67us/step\n",
      "DL 1.6435273690312509\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.5069925539344775\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 7.694297654126104e-07\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9102294367836683\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 0.8447183849366653\n",
      "\n",
      "\n",
      "--------------- Epoch 270 ---------------\n",
      "WD 0.4496323926429923\n",
      "12800/12800 [==============================] - 1s 66us/step\n",
      "DL 1.5457508760684824\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.5045718555311975\n",
      "12800/12800 [==============================] - 1s 64us/step\n",
      "DL 5.447509945000207e-07\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9087678663722463\n",
      "12800/12800 [==============================] - 1s 66us/step\n",
      "DL 0.7726630915528299\n",
      "\n",
      "\n",
      "--------------- Epoch 280 ---------------\n",
      "WD 0.4496827859099024\n",
      "12800/12800 [==============================] - 1s 70us/step\n",
      "DL 1.6320442249322782\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.5031368019603377\n",
      "12800/12800 [==============================] - 1s 66us/step\n",
      "DL 5.016165772175895e-07\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9074409857068747\n",
      "12800/12800 [==============================] - 1s 66us/step\n",
      "DL 0.8160242622453034\n",
      "\n",
      "\n",
      "--------------- Epoch 290 ---------------\n",
      "WD 0.44766886753238183\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 1.6532791028514127\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.500796931121491\n",
      "12800/12800 [==============================] - 1s 67us/step\n",
      "DL 5.097507498064147e-07\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9045605301890098\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 0.822777840954065\n",
      "\n",
      "\n",
      "--------------- Epoch 300 ---------------\n",
      "WD 0.4465607689078933\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 1.6190473958805853\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.4985272985753182\n",
      "12800/12800 [==============================] - 1s 66us/step\n",
      "DL 3.9783039763818805e-07\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9034461088991113\n",
      "12800/12800 [==============================] - 1s 70us/step\n",
      "DL 0.794308092353493\n",
      "\n",
      "\n",
      "--------------- Epoch 310 ---------------\n",
      "WD 0.4457614867464024\n",
      "12800/12800 [==============================] - 1s 66us/step\n",
      "DL 1.5458409007435836\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.4964990837210053\n",
      "12800/12800 [==============================] - 1s 66us/step\n",
      "DL 7.988437824479888e-07\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9023831306655922\n",
      "12800/12800 [==============================] - 1s 67us/step\n",
      "DL 0.7636010121846146\n",
      "\n",
      "\n",
      "--------------- Epoch 320 ---------------\n",
      "WD 0.4434943775241957\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 1.5679332907406007\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.4948187114522211\n",
      "12800/12800 [==============================] - 1s 62us/step\n",
      "DL 7.716628503118272e-07\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9002161717842125\n",
      "12800/12800 [==============================] - 1s 66us/step\n",
      "DL 0.7940857440372688\n",
      "\n",
      "\n",
      "--------------- Epoch 330 ---------------\n",
      "WD 0.44320082967493524\n",
      "12800/12800 [==============================] - 1s 67us/step\n",
      "DL 1.5746501204157772\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.4940930952085985\n",
      "12800/12800 [==============================] - 1s 64us/step\n",
      "DL 6.495528729999478e-07\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.8996265474366893\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 0.7970410163355329\n",
      "\n",
      "\n",
      "--------------- Epoch 340 ---------------\n",
      "WD 0.44123529346417767\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 1.5286426925523555\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.4917714005143892\n",
      "12800/12800 [==============================] - 1s 66us/step\n",
      "DL 7.523361380634697e-07\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.8981943967622803\n",
      "12800/12800 [==============================] - 1s 68us/step\n",
      "DL 0.771750938633608\n",
      "\n",
      "\n",
      "--------------- Epoch 350 ---------------\n",
      "WD 0.4392265605464283\n",
      "12800/12800 [==============================] - 1s 66us/step\n",
      "DL 1.3537426496138003\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.488175410837207\n",
      "12800/12800 [==============================] - 1s 67us/step\n",
      "DL 2.037948268167611e-06\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.8955573504515983\n",
      "12800/12800 [==============================] - 1s 63us/step\n",
      "DL 0.6816832369137465\n",
      "\n",
      "\n",
      "--------------- Epoch 360 ---------------\n",
      "WD 0.4388175924468949\n",
      "12800/12800 [==============================] - 1s 66us/step\n",
      "DL 1.3744497500856556\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.4874285292806573\n",
      "12800/12800 [==============================] - 1s 70us/step\n",
      "DL 1.2740745289363531e-06\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.8935351393187476\n",
      "12800/12800 [==============================] - 1s 64us/step\n",
      "DL 0.6952556249420877\n",
      "\n",
      "\n",
      "--------------- Epoch 370 ---------------\n",
      "WD 0.43690019312220074\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 1.5079133022443785\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.4850013950079384\n",
      "12800/12800 [==============================] - 1s 62us/step\n",
      "DL 8.41028808835631e-07\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.8912210544903515\n",
      "12800/12800 [==============================] - 1s 69us/step\n",
      "DL 0.7695596689426529\n",
      "\n",
      "\n",
      "--------------- Epoch 380 ---------------\n",
      "WD 0.4358877989479264\n",
      "12800/12800 [==============================] - 1s 64us/step\n",
      "DL 1.389481389670628\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.4832345102447928\n",
      "12800/12800 [==============================] - 1s 66us/step\n",
      "DL 1.1132170872230062e-06\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.8912548371493504\n",
      "12800/12800 [==============================] - 1s 62us/step\n",
      "DL 0.691868622321337\n",
      "\n",
      "\n",
      "--------------- Epoch 390 ---------------\n",
      "WD 0.43398537602651\n",
      "12800/12800 [==============================] - 1s 66us/step\n",
      "DL 1.3090081850535955\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.4811399993327514\n",
      "12800/12800 [==============================] - 1s 66us/step\n",
      "DL 1.3054400920253784e-06\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.8895625453842214\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 0.6431927120905003\n",
      "\n",
      "\n",
      "--------------- Epoch 400 ---------------\n",
      "WD 0.43339765459845075\n",
      "12800/12800 [==============================] - 1s 66us/step\n",
      "DL 1.316925567229373\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.480782643278075\n",
      "12800/12800 [==============================] - 1s 67us/step\n",
      "DL 1.4445280260844129e-06\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.8870312186252101\n",
      "12800/12800 [==============================] - 1s 69us/step\n",
      "DL 0.6480270243030354\n",
      "\n",
      "\n",
      "--------------- Epoch 410 ---------------\n",
      "WD 0.4316446363405819\n",
      "12800/12800 [==============================] - 1s 66us/step\n",
      "DL 1.131701645218056\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.477582298753167\n",
      "12800/12800 [==============================] - 1s 64us/step\n",
      "DL 2.195320340518947e-06\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.8864876131394085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 0.5653242737031288\n",
      "\n",
      "\n",
      "--------------- Epoch 420 ---------------\n",
      "WD 0.4299185283535585\n",
      "12800/12800 [==============================] - 1s 64us/step\n",
      "DL 1.1595805282309226\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.4758519307813458\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 1.4612392843105226e-06\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.8850047303097983\n",
      "12800/12800 [==============================] - 1s 66us/step\n",
      "DL 0.5804265927569884\n",
      "\n",
      "\n",
      "--------------- Epoch 430 ---------------\n",
      "WD 0.4288018658425831\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 1.194044276378792\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.4748555992475425\n",
      "12800/12800 [==============================] - 1s 65us/step\n",
      "DL 1.8348376060828286e-06\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.882992194303349\n",
      "12800/12800 [==============================] - 1s 68us/step\n",
      "DL 0.6072408884626106\n",
      "\n",
      "\n",
      "--------------- Epoch 440 ---------------\n",
      "WD 0.42797056188540294\n",
      "12800/12800 [==============================] - 1s 64us/step\n",
      "DL 1.2287896474068756\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.4722133033557017\n",
      "12800/12800 [==============================] - 1s 64us/step\n",
      "DL 2.2214212871674024e-06\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.8806975431445813\n",
      "12800/12800 [==============================] - 1s 68us/step\n",
      "DL 0.6418287695966752\n",
      "\n",
      "\n",
      "--------------- Epoch 450 ---------------\n",
      "WD 0.4271434096758256\n",
      "12800/12800 [==============================] - 1s 66us/step\n",
      "DL 1.2345850942953114\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.4710676876728161\n",
      "12800/12800 [==============================] - 1s 62us/step\n",
      "DL 1.8114851741302118e-06\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.8791724679731163\n",
      "12800/12800 [==============================] - 1s 63us/step\n",
      "DL 0.6406209816036718\n",
      "\n",
      "\n",
      "--------------- Epoch 460 ---------------\n",
      "WD 0.4265274434260196\n",
      "12800/12800 [==============================] - 1s 67us/step\n",
      "DL 1.21579476261543\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.4699679978261213\n",
      "12800/12800 [==============================] - 1s 66us/step\n",
      "DL 1.5513488241936103e-06\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.8787312515180318\n",
      "12800/12800 [==============================] - 1s 63us/step\n",
      "DL 0.6209165766189714\n",
      "\n",
      "\n",
      "--------------- Epoch 470 ---------------\n",
      "WD 0.424665413375497\n",
      "12800/12800 [==============================] - 2s 147us/step\n",
      "DL 1.1754686882997287\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.4672381300730268\n",
      "12800/12800 [==============================] - 2s 150us/step\n",
      "DL 2.5513307257796214e-06\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.87671144726779\n",
      "12800/12800 [==============================] - 2s 155us/step\n",
      "DL 0.620426172999321\n",
      "\n",
      "\n",
      "--------------- Epoch 480 ---------------\n",
      "WD 0.42380739505531795\n",
      "12800/12800 [==============================] - 2s 144us/step\n",
      "DL 1.0924822517162716\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.4669775110160048\n",
      "12800/12800 [==============================] - 2s 158us/step\n",
      "DL 3.075854755998364e-06\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.8744329772457777\n",
      "12800/12800 [==============================] - 2s 144us/step\n",
      "DL 0.5814710172140889\n",
      "\n",
      "\n",
      "--------------- Epoch 490 ---------------\n",
      "WD 0.4230772015008683\n",
      "12800/12800 [==============================] - 2s 143us/step\n",
      "DL 1.0721815625114546\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.464803061351128\n",
      "12800/12800 [==============================] - 2s 145us/step\n",
      "DL 2.0602849933482047e-06\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.8743413724550122\n",
      "12800/12800 [==============================] - 2s 148us/step\n",
      "DL 0.5615271982115765\n",
      "\n",
      "\n",
      "--------------- Epoch 500 ---------------\n",
      "WD 0.4223125375153842\n",
      "12800/12800 [==============================] - 2s 145us/step\n",
      "DL 1.0823694830965152\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.4634474367181582\n",
      "12800/12800 [==============================] - 2s 158us/step\n",
      "DL 2.3505006655000217e-06\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.8741307332083628\n",
      "12800/12800 [==============================] - 2s 151us/step\n",
      "DL 0.5696096586532821\n",
      "\n",
      "\n",
      "--------------- Epoch 510 ---------------\n",
      "WD 0.4212900181482414\n",
      "12800/12800 [==============================] - 2s 163us/step\n",
      "DL 0.9789993385777769\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.4634708087132298\n",
      "12800/12800 [==============================] - 2s 148us/step\n",
      "DL 3.078546935100235e-06\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.8713026007142828\n",
      "12800/12800 [==============================] - 2s 158us/step\n",
      "DL 0.5063383127364173\n",
      "\n",
      "\n",
      "--------------- Epoch 520 ---------------\n",
      "WD 0.4215353611114115\n",
      "12800/12800 [==============================] - 2s 152us/step\n",
      "DL 0.9905430217366127\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.4619923892109625\n",
      "12800/12800 [==============================] - 2s 164us/step\n",
      "DL 1.4515253878357726e-05\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.8698698914477014\n",
      "12800/12800 [==============================] - 2s 160us/step\n",
      "DL 0.5272941112599849\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(10000, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(0, 1, size=[len(alphas_valid[0]), INPUT_NOISE])\n",
    "\n",
    "generated = generators[0].predict([noise, np.array(alphas_valid[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  2.,   3.,  34., 105., 228., 302., 199.,  90.,  31.,   6.]),\n",
       " array([-3.41722456, -2.69952658, -1.9818286 , -1.26413062, -0.54643264,\n",
       "         0.17126534,  0.88896332,  1.60666129,  2.32435927,  3.04205725,\n",
       "         3.75975523]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "plt.hist(generated[0])\n",
    "plt.hist(batches_valid[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
    "config.gpu_options.visible_device_list = '1'\n",
    "\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, Lambda, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.objectives import binary_crossentropy, mse\n",
    "from keras.losses import mean_squared_error\n",
    "from keras import backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "tf.set_random_seed(1)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_two_head_gen(input_shape, noise_shape, n_layers, thickness, encoding_dim, optimizer):\n",
    "    input_main = Input(shape=(input_shape, ), name='input_main')\n",
    "    input_noise = Input(shape=(noise_shape, ), name='input_noise')\n",
    "    \n",
    "    x = keras.layers.concatenate([input_noise, input_main])\n",
    "    \n",
    "    for i in range(n_layers - 1):\n",
    "        x = Dense(thickness*2, activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = keras.layers.concatenate([x, input_main])\n",
    "        \n",
    "    y_1 = Dense(thickness, activation='relu')(x)\n",
    "    y_1 = BatchNormalization()(y_1)\n",
    "    y_1 = keras.layers.concatenate([y_1, input_main])\n",
    "    generated_1 = Dense(encoding_dim, name='generated_1')(x)\n",
    "    \n",
    "    \n",
    "    y_2 = Dense(thickness, activation='relu')(x)\n",
    "    y_2 = BatchNormalization()(y_2)\n",
    "    y_2 = keras.layers.concatenate([y_2, input_main])\n",
    "    generated_2 = Dense(encoding_dim, name='generated_2')(x)\n",
    "    \n",
    "    \n",
    "    gen_1 = Model([input_noise, input_main], generated_1, name=\"gen_1\")\n",
    "    gen_2 = Model([input_noise, input_main], generated_2, name=\"gen_2\")\n",
    "    \n",
    "#     optimizer_adam_1 = optimizers.Adam(lr=0.001)\n",
    "    gen_1.compile(loss='mean_squared_error', optimizer=optimizers.Adam(0.0005))\n",
    "    \n",
    "#     optimizer_adam_2 = optimizers.Adam(lr=0.001)\n",
    "    gen_2.compile(loss='mean_squared_error', optimizer=optimizers.Adam(0.0005))\n",
    "    \n",
    "    return gen_1, gen_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)\n",
    "\n",
    "def get_discriminator(input_shape, alpha_shape, n_layers, thickness, optimizer):\n",
    "    input_alpha = Input(shape=(alpha_shape, ), name='input_main')\n",
    "    inputs = Input(shape=(input_shape, ), name='input')\n",
    "    \n",
    "    x = keras.layers.concatenate([inputs, input_alpha])\n",
    "    \n",
    "    for i in range(n_layers - 1):\n",
    "        x = Dense(thickness*2, activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = keras.layers.concatenate([x, input_alpha])\n",
    "        \n",
    "    out = Dense(1, name='out')(x)\n",
    "    \n",
    "    disc = Model([inputs, input_alpha], out, name=\"disc\")\n",
    "    \n",
    "#     optimizer_adam = optimizers.Adam(lr=0.001)\n",
    "    disc.compile(loss=wasserstein_loss, optimizer=optimizers.Adam(0.0005))\n",
    "    \n",
    "    return disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gan_network(generator, discriminator, input_shape, noise_shape, optimizer):\n",
    "    for layer in discriminator.layers:\n",
    "        layer.trainable = False\n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    gan_input_main = Input(shape=(input_shape,))\n",
    "    gan_input_noise = Input(shape=(noise_shape,))\n",
    "    x = generator([gan_input_noise, gan_input_main])\n",
    "    gan_output = discriminator([x, gan_input_main] )\n",
    "    \n",
    "    gan = Model(inputs=[gan_input_noise, gan_input_main], outputs=gan_output)\n",
    "    gan.compile(loss=wasserstein_loss, optimizer=optimizers.Adam(0.0005))\n",
    "    \n",
    "    return gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input(uniform_bounds, uniform_dim, normal_dim, batch_size):\n",
    "    alpha = np.random.uniform(*uniform_bounds, size=uniform_dim)\n",
    "    return alpha, np.array([np.random.normal(alpha, 1, size=normal_dim) for i in range(batch_size)])\n",
    "\n",
    "def generate_data(bounds, batch_size, normal_dim=1024, num=10000):\n",
    "    alphas = []\n",
    "    datas = []\n",
    "\n",
    "    for i in range(num):\n",
    "        alpha, data = generate_input(bounds, 1, normal_dim, batch_size)\n",
    "        alphas.append(alpha)\n",
    "        datas.append(data.reshape((data.size, 1)))\n",
    "\n",
    "    return np.array(alphas), np.array(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha_01, data_01 = generate_data([0, 1], 1)\n",
    "# np.save(\"alpha_01_1dim\", alpha_01)\n",
    "# np.save(\"data_01_1dim\", data_01)\n",
    "\n",
    "# alpha_02, data_02 = generate_data([0, 2], 1)\n",
    "# np.save(\"alpha_02_1dim\", alpha_02)\n",
    "# np.save(\"data_02_1dim\", data_02)\n",
    "\n",
    "# alpha_12, data_12 = generate_data([1, 2], 1, num=1000)\n",
    "# np.save(\"alpha_12_1dim\", alpha_12)\n",
    "# np.save(\"data_12_1dim\", data_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1024, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(data_01).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_01 = np.load(\"alpha_01.npy\")[:5000]\n",
    "data_01 = np.load(\"data_01.npy\")[:5000]\n",
    "\n",
    "alpha_02 = np.load(\"alpha_02.npy\")[:5000]\n",
    "data_02 = np.load(\"data_02.npy\")[:5000]\n",
    "\n",
    "alpha_12 = np.load(\"alpha_12.npy\")[:5000]\n",
    "data_12 = np.load(\"data_12.npy\")[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_01.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(alphas, datas, split=False, batch_size=64, val_batch_size=1024):\n",
    "    \n",
    "    x = np.random.permutation(len(alphas))\n",
    "    alphas = alphas[x]\n",
    "    datas = datas[x]\n",
    "    \n",
    "    if not split:\n",
    "        split = 0\n",
    "    elif 0 <= split <= 1:\n",
    "        size = len(batch_alpha)\n",
    "        split = int(split*size)\n",
    "    \n",
    "    batch_alpha = []\n",
    "    batch_data = []\n",
    "    for i, alpha in enumerate(alphas[split:]):\n",
    "        for j in range(datas.shape[1]//batch_size):\n",
    "            batch_alpha.append([alpha]*batch_size)\n",
    "            batch_data.append(datas[i][j*batch_size:(j+1)*batch_size])\n",
    "    \n",
    "    val_batch_alpha = []\n",
    "    val_batch_data = []\n",
    "    for i, alpha in enumerate(alphas[:split]):\n",
    "        for j in range(datas.shape[1]//val_batch_size):\n",
    "            val_batch_alpha.append([alpha]*val_batch_size)\n",
    "            val_batch_data.append(datas[i][j*val_batch_size:(j+1)*val_batch_size])\n",
    "            \n",
    "    return np.array(batch_alpha), np.array(batch_data), np.array(val_batch_alpha), np.array(val_batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_alpha_01, batch_data_01, valid_alpha_01, valid_data_01 = prepare_data(alpha_01, data_01, batch_size=128, split=100)\n",
    "batch_alpha_02, batch_data_02, valid_alpha_02, valid_data_02 = prepare_data(alpha_02, data_02, batch_size=128, split=100)\n",
    "batch_alpha_12, batch_data_12, _, _ = prepare_data(alpha_02, data_02, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((79200, 128, 1), (100, 1024, 1))"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_alpha_02.shape, valid_alpha_02.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((79200, 128, 1), (100, 1024, 1))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data_01.shape, valid_data_01.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 1024, 1), (10000, 1024, 1))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_alpha_12.shape, batch_data_12.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = 1\n",
    "INPUT_NOISE = 1\n",
    "N_LAYERS = 3\n",
    "THICKNESS = 32\n",
    "ENCODING_DIM = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.stats\n",
    "def compare_data_to_dist(x, mus, sd=1):\n",
    "    ll = []\n",
    "    for i, item in enumerate(x):\n",
    "        ll_1 = 0\n",
    "        for j in item:\n",
    "            ll_1 += np.log(scipy.stats.norm(mus[i], sd).pdf(j))\n",
    "        ll.append(ll_1)\n",
    "    \n",
    "    print (\"The LL is: %.4f\" % (np.mean(ll)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = optimizers.Adam()\n",
    "\n",
    "generator_1, generator_2 = get_two_head_gen(input_shape=INPUT_SHAPE, noise_shape=INPUT_NOISE, \n",
    "                             n_layers=N_LAYERS, thickness=THICKNESS, encoding_dim=ENCODING_DIM, optimizer=opt)\n",
    "discriminator = get_discriminator(input_shape=ENCODING_DIM, alpha_shape=INPUT_SHAPE, n_layers=N_LAYERS, thickness=THICKNESS, optimizer=opt)\n",
    "gan_1 = get_gan_network(generator_1, discriminator, input_shape=INPUT_SHAPE, noise_shape=INPUT_NOISE, optimizer=opt)\n",
    "gan_2 = get_gan_network(generator_2, discriminator, input_shape=INPUT_SHAPE, noise_shape=INPUT_NOISE, optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(batch_alpha, batch_data):\n",
    "    while True:\n",
    "        rp = np.random.permutation(len(batch_alpha))\n",
    "        for x, y in zip(batch_alpha[rp], batch_data[rp]):\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "generators = [generator_1, generator_2]\n",
    "gans = [gan_1, gan_2]\n",
    "uniform_bounds = [[0, 1], [0, 2]]\n",
    "\n",
    "alphas_valid = [np.squeeze(valid_alpha_01), np.squeeze(valid_alpha_02)]\n",
    "batches_valid = [valid_data_01, valid_data_02]\n",
    "\n",
    "alpha_valid = batch_alpha_12[:500]\n",
    "batch_valid = batch_data_12[:500]\n",
    "\n",
    "wds = {\"01\": [], \"02\": [], \"12\": []}\n",
    "dls = {\"01\": [], \"02\": [], \"12\": []}\n",
    "\n",
    "iters_disc = 1\n",
    "iters_gen=3\n",
    "\n",
    "def valid(num):\n",
    "    for layer in discriminator.layers:\n",
    "        layer.trainable = False\n",
    "    discriminator.trainable = False\n",
    "    for layer in gans[num].layers:\n",
    "        layer.trainable = False\n",
    "    gans[num].trainable = False\n",
    "#     alpha, real = generate_input(uniform_bounds[num], INPUT_SHAPE, ENCODING_DIM, batch_size)\n",
    "    wd = []\n",
    "    for alphas, batch in zip(alphas_valid[num], batches_valid[num]):\n",
    "        noise = np.random.normal(0, 1, size=[len(alphas), INPUT_NOISE])\n",
    "        generated = generators[num].predict([noise, alphas])\n",
    "        wd.append(scipy.stats.wasserstein_distance(np.squeeze(batch), np.squeeze(generated)))\n",
    "    print(\"WD\", np.mean(wd))\n",
    "    wds[\"0\"+str(num+1)].append(np.mean(wd))\n",
    "    \n",
    "    \n",
    "    alphas = alphas_valid[num].reshape((alphas_valid[num].size, 1))\n",
    "    batch = batches_valid[num].reshape((batches_valid[num].size, 1))\n",
    "    noise = np.random.normal(0, 1, size=[len(alphas), INPUT_NOISE])\n",
    "    generated = generators[num].predict([noise, alphas])\n",
    "    to_desc = np.concatenate([batch, generated])\n",
    "\n",
    "    y_dis = np.ones(2*len(alphas))\n",
    "    y_dis[len(alphas):] = -1\n",
    "\n",
    "    \n",
    "    dl = discriminator.evaluate([to_desc, np.concatenate([alphas, alphas])], y_dis)\n",
    "    print(\"DL\", dl)\n",
    "    dls[\"0\"+str(num+1)].append(dl)\n",
    "    \n",
    "    if num == 0:\n",
    "        print(\"*****VAL [1, 2]*****\")\n",
    "        \n",
    "        wd = []\n",
    "        for alphas, batch in zip(alpha_valid, batch_valid):\n",
    "            noise = np.random.normal(0, 1, size=[len(alphas), INPUT_NOISE])\n",
    "            generated = generators[num].predict([noise, alphas])\n",
    "            wd.append(scipy.stats.wasserstein_distance(np.squeeze(batch), np.squeeze(generated)))\n",
    "        print(\"WD\", np.mean(wd))\n",
    "        wds[\"12\"].append(np.mean(wd))\n",
    "\n",
    "\n",
    "        alphas = alpha_valid.reshape((alpha_valid.size, 1))\n",
    "        batch = batch_valid.reshape((batch_valid.size, 1))\n",
    "        noise = np.random.normal(0, 1, size=[len(alphas), INPUT_NOISE])\n",
    "        generated = generators[num].predict([noise, alphas])\n",
    "        to_desc = np.concatenate([batch, generated])\n",
    "\n",
    "        y_dis = np.ones(2*len(alphas))\n",
    "        y_dis[len(alphas):] = -1\n",
    "\n",
    "        dl = discriminator.evaluate([to_desc, np.concatenate([alphas, alphas])], y_dis)\n",
    "        print(\"DL\", dl)\n",
    "        dls[\"12\"].append(dl)\n",
    "        \n",
    "        print(\"*****VAL [1, 2]*****\")\n",
    "        \n",
    "def train_desc(num, batch_alpha, batch_data):\n",
    "    batch_size = batch_alpha.shape[0]\n",
    "    noise = np.random.normal(0, 1, size=[batch_size, INPUT_NOISE])\n",
    "\n",
    "    generated = generators[num].predict([noise, batch_alpha])\n",
    "    to_desc = np.concatenate([batch_data, generated])\n",
    "\n",
    "    y_dis = np.ones(2*batch_size)\n",
    "    y_dis[batch_size:] = -1\n",
    "\n",
    "    for layer in discriminator.layers:\n",
    "        layer.trainable = True\n",
    "    discriminator.trainable = True\n",
    "    \n",
    "    discriminator.train_on_batch([to_desc, np.concatenate([batch_alpha, batch_alpha])], y_dis)\n",
    "    \n",
    "def train_gen(num, batch_alpha, batch_data):\n",
    "    for layer in gans[num].layers:\n",
    "        layer.trainable = True\n",
    "    gans[num].trainable = True\n",
    "    \n",
    "    batch_size = batch_alpha.shape[0]\n",
    "    noise = np.random.normal(0, 1, size=[batch_size, INPUT_NOISE])\n",
    "    y_gen = np.ones(batch_size)\n",
    "\n",
    "    for layer in discriminator.layers:\n",
    "        layer.trainable = False\n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    gans[num].train_on_batch([noise, batch_alpha], y_gen)\n",
    "\n",
    "def train(epochs=100, batch_size=64):\n",
    "    # Get the training and testing data\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_gen_01 = train_generator(np.squeeze(batch_alpha_01), batch_data_01)\n",
    "        train_gen_02 = train_generator(np.squeeze(batch_alpha_02), batch_data_02)\n",
    "        \n",
    "        for i in range(iters_disc):\n",
    "            batch_alpha, batch_data = next(train_gen_01)\n",
    "            train_desc(0, batch_alpha, batch_data)\n",
    "            \n",
    "        for i in range(iters_gen):\n",
    "            batch_alpha, batch_data = next(train_gen_01)\n",
    "            train_gen(0, batch_alpha, batch_data)\n",
    "            \n",
    "        for i in range(iters_disc):\n",
    "            batch_alpha, batch_data = next(train_gen_02)\n",
    "            train_desc(1, batch_alpha, batch_data)\n",
    "            \n",
    "        for i in range(iters_gen):\n",
    "            batch_alpha, batch_data = next(train_gen_02)\n",
    "            train_gen(1, batch_alpha, batch_data)\n",
    "            \n",
    "        if epoch % 50 == 0:\n",
    "            print()\n",
    "            print ('-'*15, 'Epoch %d' % epoch, '-'*15)\n",
    "            valid(0)\n",
    "            valid(1)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Epoch 50 ---------------\n",
      "WD 1.3937410642942072\n",
      "204800/204800 [==============================] - 22s 109us/step\n",
      "DL -1.5746268018701812\n",
      "*****VAL [1, 2]*****\n",
      "WD 2.6855674762125337\n",
      "1024000/1024000 [==============================] - 98s 96us/step\n",
      "DL -1.998726148098649\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.524129172600083\n",
      "204800/204800 [==============================] - 19s 95us/step\n",
      "DL -1.155473785390641\n",
      "\n",
      "\n",
      "--------------- Epoch 100 ---------------\n",
      "WD 1.206309069342421\n",
      "204800/204800 [==============================] - 20s 95us/step\n",
      "DL -1.960003142877249\n",
      "*****VAL [1, 2]*****\n",
      "WD 2.799771335475615\n",
      "1024000/1024000 [==============================] - 97s 95us/step\n",
      "DL -2.6362192650056095\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.3139029528355008\n",
      "204800/204800 [==============================] - 19s 92us/step\n",
      "DL -1.2882429854379733\n",
      "\n",
      "\n",
      "--------------- Epoch 150 ---------------\n",
      "WD 1.3277053055153945\n",
      "204800/204800 [==============================] - 19s 91us/step\n",
      "DL -3.244702857125085\n",
      "*****VAL [1, 2]*****\n",
      "WD 2.5299632666731933\n",
      "1024000/1024000 [==============================] - 92s 90us/step\n",
      "DL -1.3696129126794403\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.2985737596056461\n",
      "204800/204800 [==============================] - 18s 89us/step\n",
      "DL -1.0018355689547025\n",
      "\n",
      "\n",
      "--------------- Epoch 200 ---------------\n",
      "WD 1.423013383764264\n",
      "204800/204800 [==============================] - 18s 89us/step\n",
      "DL -5.797240799018182\n",
      "*****VAL [1, 2]*****\n",
      "WD 2.6438277214072294\n",
      "1024000/1024000 [==============================] - 91s 89us/step\n",
      "DL -2.7926666784011758\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.2898699185368934\n",
      "204800/204800 [==============================] - 17s 85us/step\n",
      "DL -1.2515211251715663\n",
      "\n",
      "\n",
      "--------------- Epoch 250 ---------------\n",
      "WD 1.1748326023535356\n",
      "204800/204800 [==============================] - 19s 92us/step\n",
      "DL -6.754767587832175\n",
      "*****VAL [1, 2]*****\n",
      "WD 2.5990241140438113\n",
      "1024000/1024000 [==============================] - 96s 94us/step\n",
      "DL -3.7673381281523035\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.2208302793709107\n",
      "204800/204800 [==============================] - 19s 95us/step\n",
      "DL -0.037656563767232004\n",
      "\n",
      "\n",
      "--------------- Epoch 300 ---------------\n",
      "WD 1.5423365058818268\n",
      "204800/204800 [==============================] - 19s 92us/step\n",
      "DL -10.380812696279026\n",
      "*****VAL [1, 2]*****\n",
      "WD 2.6125582361270916\n",
      "1024000/1024000 [==============================] - 97s 95us/step\n",
      "DL -4.259573699648492\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9614247325165214\n",
      "204800/204800 [==============================] - 20s 97us/step\n",
      "DL 4.686207403661683\n",
      "\n",
      "\n",
      "--------------- Epoch 350 ---------------\n",
      "WD 1.7360605870532932\n",
      "204800/204800 [==============================] - 20s 95us/step\n",
      "DL -16.018766685775482\n",
      "*****VAL [1, 2]*****\n",
      "WD 2.4507418201498767\n",
      "1024000/1024000 [==============================] - 97s 95us/step\n",
      "DL -11.141058075162583\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.1508957902384376\n",
      "204800/204800 [==============================] - 19s 95us/step\n",
      "DL -5.794802049226128\n",
      "\n",
      "\n",
      "--------------- Epoch 400 ---------------\n",
      "WD 1.3107678989129397\n",
      "204800/204800 [==============================] - 20s 96us/step\n",
      "DL -14.898246617764235\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.9698754017650648\n",
      "1024000/1024000 [==============================] - 97s 95us/step\n",
      "DL -4.200789841976948\n",
      "*****VAL [1, 2]*****\n",
      "WD 1.0828365998825822\n",
      "204800/204800 [==============================] - 19s 94us/step\n",
      "DL -3.5761248682718723\n",
      "\n",
      "\n",
      "--------------- Epoch 450 ---------------\n",
      "WD 1.7214201347397409\n",
      "204800/204800 [==============================] - 19s 95us/step\n",
      "DL -24.459903637589886\n",
      "*****VAL [1, 2]*****\n",
      "WD 2.3563131141354594\n",
      "1024000/1024000 [==============================] - 97s 95us/step\n",
      "DL -12.286132063255645\n",
      "*****VAL [1, 2]*****\n",
      "WD 0.9711839264528298\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-f2c84025908a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-169-695266a55f4e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, batch_size)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Epoch %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mvalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0mvalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-169-695266a55f4e>\u001b[0m in \u001b[0;36mvalid\u001b[0;34m(num)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatches_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINPUT_NOISE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mto_desc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/tgaintseva/conda/envs/py3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/mnt/tgaintseva/conda/envs/py3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/tgaintseva/conda/envs/py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/tgaintseva/conda/envs/py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`inputs` should be a list or tuple.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0mfeed_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m         \u001b[0marray_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/tgaintseva/conda/envs/py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m                     \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(10000, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(0, 1, size=[len(alphas_valid[0]), INPUT_NOISE])\n",
    "\n",
    "generated = generators[0].predict([noise, np.array(alphas_valid[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  2.,   3.,  34., 105., 228., 302., 199.,  90.,  31.,   6.]),\n",
       " array([-3.41722456, -2.69952658, -1.9818286 , -1.26413062, -0.54643264,\n",
       "         0.17126534,  0.88896332,  1.60666129,  2.32435927,  3.04205725,\n",
       "         3.75975523]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "plt.hist(generated[0])\n",
    "plt.hist(batches_valid[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
